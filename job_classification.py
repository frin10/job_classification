# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import NearestCentroid
import pandas as pd
import joblib

# Define the tokenizer globally
def split_skills_tokenizer(x):
    """Custom tokenizer to split skills by comma."""
    return x.split(',')

def preprocess_skills(df):
    df = df.copy()
    df['Skills'] = df['Skills'].fillna("").str.lower().str.strip()
    return df

def vectorize_skills(df):
    vectorizer = TfidfVectorizer(tokenizer=split_skills_tokenizer, lowercase=True)
    X = vectorizer.fit_transform(df['Skills'])
    return X, vectorizer

def cluster_skills(X, n_clusters=5):
    model = AgglomerativeClustering(n_clusters=n_clusters)
    labels = model.fit_predict(X.toarray())  # Note: must convert sparse to dense
    return model, labels

def train_centroid_classifier(X, labels):
    clf = NearestCentroid()
    clf.fit(X.toarray(), labels)
    return clf

def classify_new_jobs(df_new, vectorizer, clf):
    df_new = preprocess_skills(df_new)
    X_new = vectorizer.transform(df_new['Skills'])
    df_new['Cluster'] = clf.predict(X_new.toarray())
    return df_new

def notify_user(df_classified, user_cluster_id):
    matched = df_classified[df_classified['Cluster'] == user_cluster_id]
    if not matched.empty:
        print("üîî New job(s) matching your interest:")
        display(matched[['Title', 'Company', 'Skills']])
    else:
        print("‚ùå No new matching jobs today.")

# === PIPELINE ===

# Step 1: Scrape initial jobs
df_jobs = scrape_karkidi_jobs("data science", pages=5)

# Step 2: Preprocess
df_jobs = preprocess_skills(df_jobs)

# Step 3: Vectorize and Cluster
X, vectorizer = vectorize_skills(df_jobs)
model, labels = cluster_skills(X, n_clusters=5)
df_jobs['Cluster'] = labels

# Step 4: Train a centroid classifier for future prediction
clf = train_centroid_classifier(X, labels)

# Step 5: Simulate new jobs
df_new_jobs = scrape_karkidi_jobs("data science", pages=1)

# Step 6: Predict new job clusters
df_classified = classify_new_jobs(df_new_jobs, vectorizer, clf)

# Step 7: Notify user (assume interest in cluster 2)
notify_user(df_classified, user_cluster_id=2)

# job_clustering.py
import requests
import time
import pandas as pd
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
import joblib


def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"Scraping page: {page} -> {url}")
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
        except Exception as e:
            print(f"Failed to fetch page {page}: {e}")
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        job_blocks = soup.find_all("div", class_="ads-details")

        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company_tag = job.find("a", href=lambda x: x and "Employer-Profile" in x)
                company = company_tag.get_text(strip=True) if company_tag else "N/A"
                location = job.find("p").get_text(strip=True)
                experience_tag = job.find("p", class_="emp-exp")
                experience = experience_tag.get_text(strip=True) if experience_tag else "N/A"
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"Error parsing job block: {e}")
                continue
        time.sleep(1)
    return pd.DataFrame(jobs_list)


def preprocess_skills(df):
    df = df.copy()
    df['Skills'] = df['Skills'].fillna("").str.lower().str.strip()
    return df


def split_skills_tokenizer(x):
    return x.split(',')


def vectorize_skills(df):
    vectorizer = TfidfVectorizer(tokenizer=split_skills_tokenizer, lowercase=True)
    X = vectorizer.fit_transform(df['Skills'])
    return X, vectorizer


def cluster_skills(X, n_clusters=5):
    model = AgglomerativeClustering(n_clusters=n_clusters)
    labels = model.fit_predict(X.toarray())
    return model, labels


def classify_new_jobs(df_new, vectorizer, model):
    df_new = preprocess_skills(df_new)
    X_new = vectorizer.transform(df_new['Skills'])
    # Clustering is unsupervised, cannot predict exactly but you can retrain or compare manually
    return X_new


def notify_user(df_classified, cluster_id):
    matched = df_classified[df_classified['Cluster'] == cluster_id]
    if not matched.empty:
        print("\nüîî New matching job(s):")
        print(matched[['Title', 'Company', 'Skills']])
    else:
        print("\n‚ùå No new matching jobs found.")


if __name__ == "__main__":
    # Step 1: Scrape initial data
    df_jobs = scrape_karkidi_jobs("data science", pages=3)
    df_jobs = preprocess_skills(df_jobs)

    # Step 2: Vectorize and Cluster
    X, vectorizer = vectorize_skills(df_jobs)
    model, labels = cluster_skills(X, n_clusters=5)
    df_jobs['Cluster'] = labels

    # Save vectorizer and cluster assignments
    joblib.dump(vectorizer, "vectorizer.pkl")
    df_jobs.to_csv("clustered_jobs.csv", index=False)

    # Step 3: Scrape new jobs and assign cluster manually if needed
    df_new = scrape_karkidi_jobs("data science", pages=1)
    df_new = preprocess_skills(df_new)
    X_new = vectorizer.transform(df_new['Skills'])

    # This just uses a nearest match for manual comparison or re-clustering
    from sklearn.metrics.pairwise import cosine_similarity
    similarities = cosine_similarity(X_new, X)
    best_matches = similarities.argmax(axis=1)
    df_new['Cluster'] = df_jobs.iloc[best_matches]['Cluster'].values

    # Step 4: Notify
    notify_user(df_new, cluster_id=2)